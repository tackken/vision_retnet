{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "affine_par = True\n",
    "import functools\n",
    "\n",
    "from torch.nn import Softmax\n",
    "import timeit\n",
    "from tensorboardX import SummaryWriter\n",
    "import glob\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from typing import OrderedDict\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "import cv2\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import scipy.stats\n",
    "import slackweb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ExponentialLR\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "torch_ver = torch.__version__[:3]\n",
    "if torch_ver == '0.3':\n",
    "    from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu_num = 1\n",
    "dir_name = \"Validation_vit\"\n",
    "mode = \".\" # \".\" ---> Mac, \"kengo_workspace\" ---> Remote\n",
    "\n",
    "http_proxy = \"http://proxy.l2.med.tohoku.ac.jp:8080\"\n",
    "https_proxy = \"https://proxy.l2.med.tohoku.ac.jp:8080\"\n",
    "\n",
    "os.environ['http_proxy'] = http_proxy\n",
    "os.environ['https_proxy'] = https_proxy\n",
    "os.environ['HTTP_PROXY'] = http_proxy\n",
    "os.environ['HTTPS_PROXY'] = https_proxy\n",
    "\n",
    "http_proxy = \"http://proxy.l2.med.tohoku.ac.jp:8080\"\n",
    "https_proxy = \"https://proxy.l2.med.tohoku.ac.jp:8080\"\n",
    "\n",
    "proxies = {\n",
    "    \"http\": http_proxy,\n",
    "    \"https\": https_proxy\n",
    "}\n",
    "\n",
    "url = \"http://www.google.com\"\n",
    "response = requests.get(url, proxies=proxies)\n",
    "print(response.status_code)\n",
    "\n",
    "IMG_MEAN = np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip --proxy=http://proxy.l2.med.tohoku.ac.jp:8080 install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_bound(image, angle, flag):\n",
    "    (h, w) = image.shape[:2]\n",
    "    (cX, cY) = (w / 2, h / 2)\n",
    "\n",
    "    M = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)\n",
    "    cos = np.abs(M[0, 0])\n",
    "    sin = np.abs(M[0, 1])\n",
    "\n",
    "    nW = int((h * sin) + (w * cos))\n",
    "    nH = int((h * cos) + (w * sin))\n",
    "\n",
    "    M[0, 2] += (nW / 2) - cX\n",
    "    M[1, 2] += (nH / 2) - cY\n",
    "\n",
    "    return cv2.warpAffine(image, M, (nW, nH), flag)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_name,  # './HCC/LUNG_for_RetNet/use_for_RetNet/combo_dataset/use_for_RetNet_JSRT/image_256/Yes/JPCLN001.png'\n",
    "        label_name,  # './HCC/LUNG_for_RetNet/use_for_RetNet/combo_dataset/use_for_RetNet_JSRT/label_256/Yes/JPCLN001.png'\n",
    "        # transform,\n",
    "        scale=True,\n",
    "        crop_size=(128, 128),\n",
    "        mean=(128, 128, 128),\n",
    "        mirror=True\n",
    "    ):\n",
    "        self.image_name = image_name\n",
    "        self.label_name = label_name\n",
    "        # self.transform = transform\n",
    "        self.scale = scale\n",
    "        self.crop_h, self.crop_w = crop_size\n",
    "        self.mean = mean\n",
    "        self.is_mirror = mirror\n",
    "        \n",
    "        \n",
    "    def generate_scale_label(self, image, label):\n",
    "        f_scale = 0.35 + random.random() * 0.9\n",
    "        image = cv2.resize(image, None, fx=f_scale, fy=f_scale, interpolation=cv2.INTER_CUBIC)\n",
    "        label = cv2.resize(label, None, fx=f_scale, fy=f_scale, interpolation=cv2.INTER_NEAREST)\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_name[idx]\n",
    "        label_id = self.label_name[idx]\n",
    "        \n",
    "        filename = os.path.basename(img_id)\n",
    "        # basename = os.path.splitext(filename)[0]\n",
    "        \n",
    "        angle = -15.0 + random.random() * 30.0\n",
    "        \n",
    "        image = cv2.imread(img_id, cv2.IMREAD_COLOR) # (256, 256, 3)\n",
    "        image = rotate_bound(image, angle, cv2.INTER_CUBIC) # (286, 286, 3)\n",
    "        image = cv2.resize(image, (512, 512), interpolation=cv2.INTER_CUBIC) # (256, 256, 3)\n",
    "        \n",
    "        label = cv2.imread(label_id, cv2.IMREAD_GRAYSCALE) # (256, 256, 3)\n",
    "        label = rotate_bound(label, angle, cv2.INTER_NEAREST)\n",
    "        label = cv2.resize(label, (512, 512), interpolation=cv2.INTER_NEAREST)/255\n",
    "        \n",
    "        # 画像のサイズ取得\n",
    "        size = image.shape # (286, 286, 3)\n",
    "\n",
    "        if self.scale:\n",
    "            image, label = self.generate_scale_label(image, label)\n",
    "        image = np.asarray(image, np.float32) # (256, 256, 3)\n",
    "\n",
    "        img_h, img_w = label.shape\n",
    "        pad_h = max(self.crop_h - img_h, 0)\n",
    "        pad_w = max(self.crop_w - img_w, 0)\n",
    "        top_p = random.randint(0, pad_h)\n",
    "        left_p = random.randint(0, pad_w)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            img_pad = cv2.copyMakeBorder(image, top_p, pad_h - top_p, left_p,\n",
    "                                         pad_w - left_p, cv2.BORDER_CONSTANT,\n",
    "                                         value=(0.0, 0.0, 0.0))\n",
    "            label_pad = cv2.copyMakeBorder(label, top_p, pad_h - top_p, left_p,\n",
    "                                           pad_w - left_p, cv2.BORDER_CONSTANT,\n",
    "                                           value=(0,))\n",
    "        else:\n",
    "            img_pad, label_pad = image, label\n",
    "\n",
    "        img_pad -= self.mean\n",
    "        img_h, img_w = label_pad.shape\n",
    "        h_off = random.randint(0, img_h - self.crop_h)\n",
    "        w_off = random.randint(0, img_w - self.crop_w)\n",
    "        image = np.asarray(img_pad[h_off: h_off + self.crop_h, w_off: w_off + self.crop_w], np.float32) # (128, 128, 3)\n",
    "        label = np.asarray(label_pad[h_off: h_off + self.crop_h, w_off: w_off + self.crop_w], np.float32)\n",
    "        image = image.transpose((2, 0, 1)) # (3, 128, 128)\n",
    "        \n",
    "        \"\"\" ADD \"\"\"\n",
    "        image += 128\n",
    "        image = np.clip(image, 0, 255)\n",
    "        image = image.astype(np.uint8)\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        \"\"\" ADD \"\"\"\n",
    "        \n",
    "        if self.is_mirror:\n",
    "            flip = np.random.choice(2) * 2 - 1\n",
    "            image = image[:, :, ::flip]\n",
    "            label = label[:, ::flip]\n",
    "        label = np.expand_dims(label, axis=0)\n",
    "        \n",
    "        return torch.tensor(image.copy()), torch.tensor(label.copy()), np.array(size), filename # (3, 128, 128)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_name)\n",
    "    \n",
    "    \n",
    "####################################################################################################################################################################################################\n",
    "class MyDataset_Test(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_name,  # './HCC/LUNG_for_RetNet/use_for_RetNet/combo_dataset/use_for_RetNet_JSRT/image_256/Yes/JPCLN001.png'\n",
    "        label_name,  # './HCC/LUNG_for_RetNet/use_for_RetNet/combo_dataset/use_for_RetNet_JSRT/label_256/Yes/JPCLN001.png'\n",
    "        # transform,\n",
    "        scale=True,\n",
    "        crop_size=(128, 128),\n",
    "        mean=(128, 128, 128),\n",
    "        mirror=True\n",
    "    ):\n",
    "        self.image_name = image_name\n",
    "        self.label_name = label_name\n",
    "        # self.transform = transform\n",
    "        self.scale = scale\n",
    "        self.crop_h, self.crop_w = crop_size\n",
    "        self.mean = mean\n",
    "        self.is_mirror = mirror\n",
    "        \n",
    "        \n",
    "    def generate_scale_label(self, image, label):\n",
    "        f_scale = 0.35 + random.random() * 0.9\n",
    "        image = cv2.resize(image, None, fx=f_scale, fy=f_scale, interpolation=cv2.INTER_CUBIC)\n",
    "        label = cv2.resize(label, None, fx=f_scale, fy=f_scale, interpolation=cv2.INTER_NEAREST)\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_name[idx]\n",
    "        label_id = self.label_name[idx]\n",
    "        \n",
    "        filename = os.path.basename(img_id)\n",
    "        \n",
    "        filename = os.path.basename(img_id)\n",
    "        basename = os.path.splitext(filename)[0]\n",
    "        \n",
    "        angle = -15.0 + random.random() * 30.0\n",
    "        \n",
    "        image = cv2.imread(img_id, cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image, (512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "        size = image.shape\n",
    "        image = np.asarray(image, np.float32)\n",
    "        image -= self.mean\n",
    "        \n",
    "        img_h, img_w, _ = image.shape\n",
    "        pad_h = max(self.crop_h - img_h, 0)\n",
    "        pad_w = max(self.crop_w - img_w, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            image = cv2.copyMakeBorder(image, 0, pad_h, 0,\n",
    "                                       pad_w, cv2.BORDER_CONSTANT,\n",
    "                                       value=(0.0, 0.0, 0.0))\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        \n",
    "        \"\"\" ADD \"\"\"\n",
    "        image += 128\n",
    "        image = np.clip(image, 0, 255)\n",
    "        image = image.astype(np.uint8)\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        \"\"\" ADD \"\"\"\n",
    "        \n",
    "        return image, np.array(size), filename\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = sorted(glob.glob(f'{mode}/HCC/LUNG_for_RetNet/combo_dataset/*/image_512/*/*.png'))\n",
    "label_name = sorted(glob.glob(f'{mode}/HCC/LUNG_for_RetNet/combo_dataset/*/label_512/*/*.png'))\n",
    "\n",
    "# image = cv2.imread(image_name[0], cv2.IMREAD_COLOR)\n",
    "# image = cv2.resize(image, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "# label = cv2.imread(label_name[0], cv2.IMREAD_GRAYSCALE) # (256, 256, 3)\n",
    "# # Matplotlibを使って画像とラベルを表示\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# # 画像の表示\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # BGRをRGBに変換して表示\n",
    "# plt.title('Image')\n",
    "# plt.axis('off')\n",
    "\n",
    "# # ラベルの表示\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(label, cmap='gray')  # グレースケールとしてラベルを表示\n",
    "# plt.title('Label')\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "length = len(image_name) # 385\n",
    "list_id = list(range(0, length)) # [0, 1, 2, 3, 4, 5, ... , 380, 381, 382, 383, 384]\n",
    "\n",
    "seed_value = 8\n",
    "random.seed(seed_value)\n",
    "\n",
    "\"\"\" Train \"\"\"\n",
    "train_ID = random.sample(list_id, 270)\n",
    "image_train = []\n",
    "label_train = []\n",
    "for index in train_ID:\n",
    "    image_train.append(image_name[index])\n",
    "    label_train.append(label_name[index])\n",
    "\n",
    "\"\"\" Validation \"\"\"\n",
    "remaining_list = list(set(list_id) - set(train_ID))\n",
    "validation_ID = random.sample(remaining_list, 40)\n",
    "image_validation = []\n",
    "label_validation = []\n",
    "for index in validation_ID:\n",
    "    image_validation.append(image_name[index])\n",
    "    label_validation.append(label_name[index])\n",
    "    \n",
    "\"\"\" Test \"\"\"\n",
    "remaining_list = list(set(list_id) - set(train_ID) - set(validation_ID))\n",
    "test_ID = random.sample(remaining_list, 40)\n",
    "image_test = []\n",
    "label_test = []\n",
    "for index in test_ID:\n",
    "    image_test.append(image_name[index])\n",
    "    label_test.append(label_name[index])\n",
    "\n",
    "\"\"\" Make MyDataset for each dataset\"\"\"\n",
    "# Train\n",
    "train_set = MyDataset(\n",
    "    image_train,\n",
    "    label_train,\n",
    "    # transform=False,\n",
    "    scale=True,\n",
    "    crop_size=(512, 512),\n",
    "    mean=(128, 128, 128),\n",
    "    mirror=True\n",
    ")\n",
    "\n",
    "# Validation\n",
    "validation_set = MyDataset(\n",
    "    image_validation,\n",
    "    label_validation,\n",
    "    # transform=False,\n",
    "    scale=False,\n",
    "    crop_size=(512, 512),\n",
    "    mean=(128, 128, 128),\n",
    "    mirror=False\n",
    ")\n",
    "\n",
    "# Test\n",
    "test_set = MyDataset_Test(\n",
    "    image_test,\n",
    "    label_test,\n",
    "    # transform=False,\n",
    "    scale=False,\n",
    "    crop_size=(512, 512),\n",
    "    mean=(128, 128, 128),\n",
    "    mirror=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------\")\n",
    "total_images_train_set = len(train_set)\n",
    "print(f\"Total number of images(Train): {total_images_train_set}　\")\n",
    "total_images_validation_set = len(validation_set)\n",
    "print(f\"Total number of images(Validation): {total_images_validation_set}\")\n",
    "total_images_test_set = len(test_set)\n",
    "print(f\"Total number of images(Test): {total_images_test_set})\")\n",
    "print(\"--------------\")\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "validationloader = torch.utils.data.DataLoader(\n",
    "    validation_set, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------- Model --------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def INF(B,H,W):\n",
    "     return -torch.diag(torch.tensor(float(\"inf\")).cuda(gpu_num).repeat(H),0).unsqueeze(0).repeat(B*W,1,1)\n",
    "\n",
    "\n",
    "class CrissCrossAttention(nn.Module):\n",
    "    \"\"\" Criss-Cross Attention Module\"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(CrissCrossAttention,self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.softmax = Softmax(dim=3)\n",
    "        self.INF = INF\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        m_batchsize, _, height, width = x.size()\n",
    "        proj_query = self.query_conv(x)\n",
    "        proj_query_H = proj_query.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height).permute(0, 2, 1)\n",
    "        proj_query_W = proj_query.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x)\n",
    "        proj_key_H = proj_key.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height)\n",
    "        proj_key_W = proj_key.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width)\n",
    "        proj_value = self.value_conv(x)\n",
    "        proj_value_H = proj_value.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height)\n",
    "        proj_value_W = proj_value.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width)\n",
    "        energy_H = (torch.bmm(proj_query_H, proj_key_H)+self.INF(m_batchsize, height, width)).view(m_batchsize,width,height,height).permute(0,2,1,3)\n",
    "        energy_W = torch.bmm(proj_query_W, proj_key_W).view(m_batchsize,height,width,width)\n",
    "        concate = self.softmax(torch.cat([energy_H, energy_W], 3))\n",
    "\n",
    "        att_H = concate[:,:,:,0:height].permute(0,2,1,3).contiguous().view(m_batchsize*width,height,height)\n",
    "        att_W = concate[:,:,:,height:height+width].contiguous().view(m_batchsize*height,width,width)\n",
    "        out_H = torch.bmm(proj_value_H, att_H.permute(0, 2, 1)).view(m_batchsize,width,-1,height).permute(0,2,3,1)\n",
    "        out_W = torch.bmm(proj_value_W, att_W.permute(0, 2, 1)).view(m_batchsize,height,-1,width).permute(0,2,1,3)\n",
    "        return self.gamma*(out_H + out_W) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm2d = functools.partial(InPlaceABNSync, activation='none')\n",
    "BatchNorm2d = nn.BatchNorm2d\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None, fist_dilation=1, multi_grid=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=dilation*multi_grid, dilation=dilation*multi_grid, bias=False)\n",
    "        self.bn2 = BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.relu_inplace = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.dilation = dilation\n",
    "        self.stride = stride\n",
    "    \n",
    "    def _sum_each(self, x, y):\n",
    "        assert(len(x)==len(y))\n",
    "        z = []\n",
    "        for i in range(len(x)):\n",
    "            z.append(x[i]+y[i])\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out = out + residual      \n",
    "        out = self.relu_inplace(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class RCCAModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_classes):\n",
    "        super(RCCAModule, self).__init__()\n",
    "        inter_channels = in_channels // 4\n",
    "        self.conv1a = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
    "                                   BatchNorm2d(inter_channels))\n",
    "        self.cca = CrissCrossAttention(inter_channels)\n",
    "\n",
    "        self.conv1b = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n",
    "                                   BatchNorm2d(inter_channels))\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(in_channels+inter_channels, out_channels, kernel_size=3, padding=1, dilation=1, bias=False),\n",
    "            BatchNorm2d(out_channels),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.Conv2d(out_channels, num_classes, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, recurrence=1):\n",
    "        output = self.conv1a(x)\n",
    "        for i in range(recurrence):\n",
    "            output = self.cca(output)\n",
    "        output = self.conv1b(output)\n",
    "\n",
    "        output = self.bottleneck(torch.cat([x, output], 1))\n",
    "        return output\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes):\n",
    "        self.inplanes = 128\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = conv3x3(3, 64, stride=2)\n",
    "        self.bn1 = BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        self.conv2 = conv3x3(64, 64)\n",
    "        self.bn2 = BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU(inplace=False)\n",
    "        self.conv3 = conv3x3(64, 128)\n",
    "        self.bn3 = BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU(inplace=False)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True) # change\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4, multi_grid=(1,1,1))\n",
    "        self.head = RCCAModule(2048, 512, num_classes)  ################# CrissCrossAttention ####################\n",
    "\n",
    "        self.dsn = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(512),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.Conv2d(512, num_classes, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "            )\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, multi_grid=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                BatchNorm2d(planes * block.expansion,affine = affine_par))\n",
    "\n",
    "        layers = []\n",
    "        generate_multi_grid = lambda index, grids: grids[index%len(grids)] if isinstance(grids, tuple) else 1\n",
    "        layers.append(block(self.inplanes, planes, stride,dilation=dilation, downsample=downsample, multi_grid=generate_multi_grid(0, multi_grid)))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, dilation=dilation, multi_grid=generate_multi_grid(i, multi_grid)))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, recurrence=1):\n",
    "#         print(f\"x_0: {x.shape}\")                # [32, 3, 128, 128]\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "#         print(f\"relu1: {x.shape}\")                # [32, 64, 64, 64]\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "#         print(f\"relu2: {x.shape}\")                # [32, 64, 64, 64]\n",
    "        x = self.relu3(self.bn3(self.conv3(x)))\n",
    "#         print(f\"relu3: {x.shape}\")                # [32, 128, 64, 64]\n",
    "        x = self.maxpool(x)\n",
    "#         print(f\"x_maxpool: {x.shape}\")        # [32, 128, 33, 33]\n",
    "        x = self.layer1(x)\n",
    "#         print(f\"layer1: {x.shape}\")        # [32, 256, 33, 33]\n",
    "        x = self.layer2(x)\n",
    "#         print(f\"layer2: {x.shape}\")        # [32, 512, 17, 17]\n",
    "        x = self.layer3(x)\n",
    "#         print(f\"layer3: {x.shape}\")        # [32, 1024, 17, 17]\n",
    "        x_dsn = self.dsn(x)\n",
    "#         print(f\"x_dsn: {x_dsn.shape}\")        # [32, 2, 17, 17],   [32, 2, 17, 17]\n",
    "        x = self.layer4(x)\n",
    "#         print(f\"layer4: {x.shape}\")        # [32, 2, 17, 17],   [32, 2048, 17, 17]\n",
    "        x = self.head(x, recurrence) ################# CrissCrossAttention ####################\n",
    "#         print(f\"x_lst: {x.shape}\")       # [32, 2, 17, 17],     [32, 2, 17, 17]\n",
    "        return [x, x_dsn]\n",
    "\n",
    "def XLSor(num_classes=2):\n",
    "    model = ResNet(Bottleneck,[3, 4, 23, 3], num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Criterion(nn.Module):\n",
    "    def __init__(self, ignore_index=255, use_weight=True, reduce=True):\n",
    "        super(Criterion, self).__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.criterion = torch.nn.MSELoss(size_average=True)\n",
    "        if not reduce:\n",
    "            print(\"disabled the reduce.\")\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        h, w = target.size(2), target.size(3) # 128, 128\n",
    "\n",
    "#         print(f\"preds: {preds}\")\n",
    "        scale_pred = F.upsample(input=preds[0], size=(h, w), mode='bilinear', align_corners=True)\n",
    "#         print(f\"scale_pred: {scale_pred.shape}\") # [32, 2, 128, 128]\n",
    "#         print(f\"target: {target.shape}\") # [32, 1, 128, 128, 3]\n",
    "        loss1 = self.criterion(scale_pred, target)\n",
    "\n",
    "        scale_pred = F.upsample(input=preds[1], size=(h, w), mode='bilinear', align_corners=True)\n",
    "        loss2 = self.criterion(scale_pred, target)\n",
    "\n",
    "        return loss1 + loss2*0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsor = XLSor(num_classes=2)\n",
    "# print(xlsor)\n",
    "\n",
    "model = xlsor\n",
    "model.train()\n",
    "model.float()\n",
    "model.cuda(gpu_num)    \n",
    "\n",
    "criterion = Criterion()\n",
    "criterion.cuda(gpu_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "    imgs, labels, _, _  = data\n",
    "    # imgs: torch.Size([32, 3, 128, 128])\n",
    "    # labels: torch.Size([32, 1, 128, 128])\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    if i > 5:\n",
    "        print(\"------ Image ------\")\n",
    "        img = torchvision.utils.make_grid(imgs).numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = img[:, :, ::-1]\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    if i > 5:\n",
    "        print(\"------ Label ------\")\n",
    "        img = torchvision.utils.make_grid(labels).numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = img[:, :, ::-1]\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------- Train Start --------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS = 5000\n",
    "NUM_EPOCHS = 5\n",
    "SAVE_PRED_EVERY = 50\n",
    "\n",
    "def lr_poly(base_lr, iter, max_iter, power):\n",
    "    return base_lr*((1-float(iter)/max_iter)**(power))\n",
    "\n",
    "def adjust_learning_rate(optimizer, i_iter):\n",
    "    lr = lr_poly(1e-2, i_iter, NUM_STEPS, 0.9)\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def inv_preprocess(imgs, num_images, img_mean):\n",
    "    \"\"\"Inverse preprocessing of the batch of images.\n",
    "       Add the mean vector and convert from BGR to RGB.\n",
    "       \n",
    "    Args:\n",
    "      imgs: batch of input images.\n",
    "      num_images: number of images to apply the inverse transformations on.\n",
    "      img_mean: vector of mean colour values.\n",
    "  \n",
    "    Returns:\n",
    "      The batch of the size num_images with the same spatial dimensions as the input.\n",
    "    \"\"\"\n",
    "    imgs = imgs.data.cpu().numpy()\n",
    "    n, c, h, w = imgs.shape\n",
    "    assert(n >= num_images), 'Batch size %d should be greater or equal than number of images to save %d.' % (n, num_images)\n",
    "    outputs = np.zeros((num_images, h, w, c), dtype=np.uint8)\n",
    "    for i in range(num_images):\n",
    "        outputs[i] = (np.transpose(imgs[i], (1,2,0)) + img_mean).astype(np.uint8)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "start_iters = 0\n",
    "start = timeit.default_timer()\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=0.02)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=1e-5, nesterov=True)\n",
    "\n",
    "pathy = './HCC/aaa/'\n",
    "writer = SummaryWriter(pathy)\n",
    "\n",
    "interp = nn.Upsample(size=(256, 256), mode='bilinear', align_corners=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i_iter, batch in enumerate(trainloader):\n",
    "        i_iter += start_iters + epoch * len(trainloader)\n",
    "        images, labels, _, _ = batch\n",
    "        images = images.cuda(gpu_num)\n",
    "        labels = labels.float().cuda(gpu_num)\n",
    "\n",
    "        if torch_ver == \"0.3\":\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        lr = adjust_learning_rate(optimizer, i_iter)\n",
    "        preds = model(images, 4)\n",
    "\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_iter % 50 == 0:\n",
    "            writer.add_scalar('learning_rate', lr, i_iter)\n",
    "            writer.add_scalar('loss', loss.data.cpu().numpy(), i_iter)\n",
    "\n",
    "        if i_iter % 50 == 0:\n",
    "            images_inv = inv_preprocess(images, 2, IMG_MEAN)\n",
    "            if isinstance(preds, list):\n",
    "                preds = preds[0]\n",
    "            if isinstance(preds, list):\n",
    "                preds = preds[0]\n",
    "            preds = interp(preds)\n",
    "            for index, img in enumerate(images_inv):\n",
    "                writer.add_image('Images/'+str(index), torch.from_numpy(img/255.).permute(2,0,1), i_iter)\n",
    "                writer.add_image('Labels/'+str(index), labels[index], i_iter)\n",
    "                writer.add_image('preds/'+str(index), (preds[index]>0.5).float(), i_iter)\n",
    "\n",
    "        print('iter = {} of {} completed, loss = {}'.format(i_iter, NUM_STEPS, loss.data.cpu().numpy()))\n",
    "\n",
    "        if i_iter >= NUM_STEPS-1:\n",
    "            print('save model ...')\n",
    "            torch.save(xlsor.state_dict(), osp.join(pathy, 'XLSor_'+str(NUM_STEPS)+'.pth'))\n",
    "            break\n",
    "\n",
    "        if i_iter % SAVE_PRED_EVERY == 0:\n",
    "            print('taking snapshot ...')\n",
    "            torch.save(xlsor.state_dict(), osp.join(pathy, 'XLSor_'+str(i_iter)+'.pth'))\n",
    "\n",
    "    if i_iter >= NUM_STEPS-1:\n",
    "        break\n",
    "\n",
    "end = timeit.default_timer()\n",
    "print(end-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------- Test Start --------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XLSor(num_classes=2)\n",
    "\n",
    "saved_state_dict = torch.load(f\"{pathy}/XLSor_200.pth\")\n",
    "model.load_state_dict(saved_state_dict)\n",
    "\n",
    "model.eval()\n",
    "model.cuda(gpu_num)\n",
    "\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "\n",
    "output_buffer = []\n",
    "for index, batch in enumerate(testloader):\n",
    "    if index % 100 == 0:\n",
    "        print('%d processd'%(index))\n",
    "    image, size, name = batch # [32, 3, 512, 512]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model(image.cuda(gpu_num), 2)\n",
    "        if isinstance(prediction, list):\n",
    "            prediction = prediction[0]\n",
    "            print(prediction.shape)\n",
    "        prediction = interp(prediction).cpu().data[0].numpy().transpose(1, 2, 0) # (256, 256, 2)\n",
    "        \n",
    "    # Default\n",
    "    # output_im = PILImage.fromarray((np.clip(prediction[:,:,0],0,1)* 255).astype(np.uint8))\n",
    "    \n",
    "    # > 0.5: 閾値を0.5として、クリップした結果が0.5を超える場合はTrue (1)、それ以外はFalse (0) を表すブール値配列を作成します。\n",
    "    output_im = (np.clip(prediction[:,:,0], 0, 1) > 0.5).astype(np.uint8) * 255\n",
    "    output_im = PILImage.fromarray(output_im)\n",
    "    \n",
    "    output_im.save('./outputs/' + name[0].replace('.png', '_xlsor.png'), 'png')\n",
    "    \n",
    "    ###\n",
    "    #output_im_resized = output_im.resize((512, 512))  # 必要に応じてリサイズ\n",
    "    output_buffer.append(output_im)\n",
    "    ###\n",
    "\n",
    "combined_image = Image.new('RGB', (256 * len(output_buffer), 256))\n",
    "for i, im in enumerate(output_buffer):\n",
    "    combined_image.paste(im, (i * 256, 0))\n",
    "\n",
    "combined_image.save('combined_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
